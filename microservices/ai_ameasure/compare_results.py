#!/usr/bin/env python3
"""
Streamlit GUI vs API Backend Comparison Tool

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯Streamlitã®GUIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨APIãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®
å­¦ç¿’çµæœã‚’æ¯”è¼ƒã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

æ¯”è¼ƒé …ç›®ï¼š
1. å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆRÂ²ã‚¹ã‚³ã‚¢ã€MSEï¼‰ã®å®Œå…¨ä¸€è‡´
2. å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.pklï¼‰ã®å­˜åœ¨ç¢ºèª

ã“ã‚Œã‚‰ã®æ¡ä»¶ã‚’æº€ãŸã›ã°æˆåŠŸã¨åˆ¤å®šã—ã¾ã™ã€‚
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.metrics import r2_score, mean_squared_error
import requests
import time
import hashlib
from typing import Dict, List, Tuple, Any

# Streamlitãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append('/home/nowatari/repos/enzan-ai-cn-dev/ai_ameasure')

class ResultComparator:
    def __init__(self):
        self.streamlit_results = {}
        self.api_results = {}
        self.comparison_report = {}
        
        # è¨­å®š
        self.folder_name = "01-hokkaido-akan" 
        self.model_name = "Random Forest"
        self.td = 500
        self.max_distance_from_face = 100
        self.api_url = "http://localhost:8000/api/v1/displacement-analysis"
        
        # å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€
        self.streamlit_output = "./output_streamlit_test"
        self.api_output = "./output_api_test"
        
    def run_streamlit_analysis(self) -> Dict[str, Any]:
        """Streamlitã®åˆ†æã‚’å®Ÿè¡Œã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        print("ğŸ”„ Running Streamlit analysis...")
        
        try:
            # Streamlitã®é–¢æ•°ã‚’ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ»å®Ÿè¡Œ
            from app.displacement_temporal_spacial_analysis import analyze_displacement
            from sklearn.ensemble import RandomForestRegressor
            
            # Streamlitã¨åŒã˜ãƒ¢ãƒ‡ãƒ«è¨­å®š
            models = {
                "Random Forest": RandomForestRegressor(random_state=42),
            }
            
            input_folder = f'/home/nowatari/repos/enzan-ai-cn-dev/data_folder/{self.folder_name}/main_tunnel/CN_measurement_data'
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®è¨­å®š
            model_paths = {
                "final_value_prediction_model": [
                    os.path.join(self.streamlit_output, "model_final_settlement.pkl"),
                    os.path.join(self.streamlit_output, "model_final_convergence.pkl")
                ],
                "prediction_model": [
                    os.path.join(self.streamlit_output, "model_settlement.pkl"),
                    os.path.join(self.streamlit_output, "model_convergence.pkl")
                ]
            }
            
            # åˆ†æå®Ÿè¡Œ
            result = analyze_displacement(
                input_folder,
                self.streamlit_output,
                model_paths,
                models[self.model_name],
                self.max_distance_from_face,
                td=self.td
            )
            
            # æˆ»ã‚Šå€¤ã®å‡¦ç†
            if isinstance(result, tuple):
                if len(result) == 3:
                    df_all, training_metrics, scatter_data = result
                elif len(result) == 2:
                    df_all, training_metrics = result
                    scatter_data = {}
                else:
                    df_all = result[0] if result else None
                    training_metrics = {}
                    scatter_data = {}
            else:
                df_all = result
                training_metrics = {}
                scatter_data = {}
            
            # çµæœã®ä¿å­˜
            streamlit_result = {
                'training_metrics': training_metrics,
                'scatter_data': scatter_data,
                'output_files': self._get_output_files(self.streamlit_output),
                'execution_time': time.time()
            }
            
            print("âœ… Streamlit analysis completed")
            return streamlit_result
            
        except Exception as e:
            print(f"âŒ Streamlit analysis failed: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def run_api_analysis(self) -> Dict[str, Any]:
        """APIã®åˆ†æã‚’å®Ÿè¡Œã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        print("ğŸ”„ Running API analysis...")
        
        try:
            # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            response = requests.post(
                f"{self.api_url}/analyze-whole",
                headers={"Content-Type": "application/json"},
                json={
                    "folder_name": self.folder_name,
                    "model_name": self.model_name,
                    "td": self.td,
                    "max_distance_from_face": self.max_distance_from_face
                },
                timeout=300  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            )
            
            if response.status_code == 200:
                api_result = response.json()
                api_result['output_files'] = self._get_output_files('./output')
                api_result['execution_time'] = time.time()
                
                print("âœ… API analysis completed")
                print(f"   Status: {api_result.get('status', 'unknown')}")
                print(f"   Message: {api_result.get('message', 'no message')}")
                print(f"   Model files saved: {api_result.get('model_files_saved', False)}")
                return api_result
            else:
                print(f"âŒ API request failed: {response.status_code} - {response.text}")
                return {'error': response.text}
                
        except requests.exceptions.Timeout:
            print("âŒ API request timed out")
            return {'error': 'timeout'}
        except Exception as e:
            print(f"âŒ API analysis failed: {e}")
            return {'error': str(e)}
    
    def _get_output_files(self, output_dir: str) -> Dict[str, Any]:
        """å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—"""
        files_info = {}
        
        if os.path.exists(output_dir):
            for file_path in Path(output_dir).rglob("*"):
                if file_path.is_file():
                    rel_path = str(file_path.relative_to(output_dir))
                    files_info[rel_path] = {
                        'size': file_path.stat().st_size,
                        'exists': True
                    }
                    
                    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤è¨ˆç®—ï¼ˆå†…å®¹ã®åŒä¸€æ€§ç¢ºèªï¼‰
                    if file_path.suffix == '.csv':
                        try:
                            with open(file_path, 'rb') as f:
                                files_info[rel_path]['hash'] = hashlib.md5(f.read()).hexdigest()
                        except:
                            files_info[rel_path]['hash'] = None
        
        return files_info
    
    def compare_metrics(self) -> Dict[str, Any]:
        """å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ¯”è¼ƒ"""
        print("ğŸ”„ Comparing metrics...")
        
        comparison = {
            'metrics_match': True,
            'differences': [],
            'streamlit_metrics': self.streamlit_results.get('training_metrics', {}),
            'api_metrics': self.api_results.get('training_metrics', {}),
        }
        
        # Streamlitã®çµæœã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
        streamlit_metrics = self._extract_metrics_from_csv(self.streamlit_output)
        
        # APIã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ï¼ˆAPIãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’è€ƒæ…®ï¼‰
        api_training_metrics = self.api_results.get('training_metrics', {})
        print(f"ğŸ“Š API training_metrics keys: {list(api_training_metrics.keys()) if api_training_metrics else 'None'}")
        
        # APIãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’æ¨™æº–å½¢å¼ã«å¤‰æ›
        api_metrics = self._convert_api_metrics_format(api_training_metrics)
        print(f"ğŸ“Š Converted API metrics keys: {list(api_metrics.keys()) if api_metrics else 'None'}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
        if not api_metrics:
            api_metrics = self._extract_metrics_from_csv('./output')
        
        comparison['calculated_streamlit_metrics'] = streamlit_metrics
        comparison['calculated_api_metrics'] = api_metrics
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¯”è¼ƒ
        for metric_type in ['settlement', 'convergence']:
            if metric_type in streamlit_metrics and metric_type in api_metrics:
                s_metrics = streamlit_metrics[metric_type]
                a_metrics = api_metrics[metric_type]
                
                for key in ['r2_train', 'r2_validate', 'mse_train', 'mse_validate']:
                    if key in s_metrics and key in a_metrics:
                        s_val = s_metrics[key]
                        a_val = a_metrics[key]
                        diff = abs(s_val - a_val)
                        
                        if diff > 1e-6:  # è¨±å®¹èª¤å·®
                            comparison['metrics_match'] = False
                            comparison['differences'].append({
                                'metric': f"{metric_type}_{key}",
                                'streamlit': s_val,
                                'api': a_val,
                                'difference': diff
                            })
        
        return comparison
    
    def _convert_api_metrics_format(self, api_training_metrics: Dict[str, Any]) -> Dict[str, Any]:
        """
        APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹å½¢å¼ã‚’æ¨™æº–å½¢å¼ã«å¤‰æ›
        
        APIå½¢å¼ï¼š
        {
            "æœ€çµ‚æ²ˆä¸‹é‡ã¨ã®å·®åˆ†": {"mse_train": ..., "r2_train": ..., ...},
            "æœ€çµ‚å¤‰ä½é‡ã¨ã®å·®åˆ†": {"mse_train": ..., "r2_train": ..., ...}
        }
        
        æ¨™æº–å½¢å¼ï¼š
        {
            "settlement": {"mse_train": ..., "r2_train": ..., ...},
            "convergence": {"mse_train": ..., "r2_train": ..., ...}
        }
        """
        if not api_training_metrics:
            return {}
        
        converted_metrics = {}
        
        # æ²ˆä¸‹é‡é–¢é€£ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¤‰æ›
        if "æœ€çµ‚æ²ˆä¸‹é‡ã¨ã®å·®åˆ†" in api_training_metrics:
            converted_metrics["settlement"] = api_training_metrics["æœ€çµ‚æ²ˆä¸‹é‡ã¨ã®å·®åˆ†"]
        
        # å¤‰ä½é‡é–¢é€£ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¤‰æ›
        if "æœ€çµ‚å¤‰ä½é‡ã¨ã®å·®åˆ†" in api_training_metrics:
            converted_metrics["convergence"] = api_training_metrics["æœ€çµ‚å¤‰ä½é‡ã¨ã®å·®åˆ†"]
        
        return converted_metrics
    
    def _extract_metrics_from_csv(self, output_dir: str) -> Dict[str, Any]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å®Ÿéš›ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º"""
        metrics = {}
        
        # æ²ˆä¸‹é‡çµæœ
        settlement_csv = os.path.join(output_dir, 'resultæœ€çµ‚æ²ˆä¸‹é‡ã¨ã®å·®åˆ†.csv')
        if os.path.exists(settlement_csv):
            try:
                df = pd.read_csv(settlement_csv)
                train_df = df[df['mode'] == 'train']
                val_df = df[df['mode'] == 'validate']
                y_col = 'æœ€çµ‚æ²ˆä¸‹é‡ã¨ã®å·®åˆ†'
                
                if not train_df.empty and not val_df.empty:
                    metrics['settlement'] = {
                        'r2_train': float(r2_score(train_df[y_col], train_df['pred'])),
                        'r2_validate': float(r2_score(val_df[y_col], val_df['pred'])),
                        'mse_train': float(mean_squared_error(train_df[y_col], train_df['pred'])),
                        'mse_validate': float(mean_squared_error(val_df[y_col], val_df['pred'])),
                        'train_samples': len(train_df),
                        'validate_samples': len(val_df)
                    }
            except Exception as e:
                print(f"Error extracting settlement metrics: {e}")
        
        # å¤‰ä½é‡çµæœ
        convergence_csv = os.path.join(output_dir, 'resultæœ€çµ‚å¤‰ä½é‡ã¨ã®å·®åˆ†.csv')
        if os.path.exists(convergence_csv):
            try:
                df = pd.read_csv(convergence_csv)
                train_df = df[df['mode'] == 'train']
                val_df = df[df['mode'] == 'validate']
                y_col = 'æœ€çµ‚å¤‰ä½é‡ã¨ã®å·®åˆ†'
                
                if not train_df.empty and not val_df.empty:
                    metrics['convergence'] = {
                        'r2_train': float(r2_score(train_df[y_col], train_df['pred'])),
                        'r2_validate': float(r2_score(val_df[y_col], val_df['pred'])),
                        'mse_train': float(mean_squared_error(train_df[y_col], train_df['pred'])),
                        'mse_validate': float(mean_squared_error(val_df[y_col], val_df['pred'])),
                        'train_samples': len(train_df),
                        'validate_samples': len(val_df)
                    }
            except Exception as e:
                print(f"Error extracting convergence metrics: {e}")
        
        return metrics
    
    def compare_files(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.pklï¼‰ã®å­˜åœ¨ã®ã¿ã‚’ç¢ºèª"""
        print("ğŸ”„ Checking model files...")
        
        api_files = self.api_results.get('output_files', {})
        
        # å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
        required_model_files = [
            'model_final_settlement.pkl',
            'model_final_convergence.pkl', 
            'model_settlement.pkl',
            'model_convergence.pkl'
        ]
        
        comparison = {
            'files_match': True,
            'missing_model_files': [],
            'present_model_files': []
        }
        
        # APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ç›´æ¥ç¢ºèªï¼ˆã‚ˆã‚Šç¢ºå®Ÿï¼‰
        api_model_files_saved = self.api_results.get('model_files_saved', False)
        
        print(f"ğŸ“‹ API model_files_saved status: {api_model_files_saved}")
        
        if api_model_files_saved:
            # APIãŒæˆåŠŸã‚’å ±å‘Šã—ã¦ã„ã‚‹å ´åˆã€å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã¨åˆ¤æ–­
            comparison['present_model_files'] = required_model_files.copy()
            comparison['files_match'] = True
            print("âœ… Using API response status: All model files reported as saved")
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã§å€‹åˆ¥ç¢ºèª
            print("âš ï¸ API did not report model files as saved, checking filesystem...")
            for model_file in required_model_files:
                if model_file in api_files:
                    comparison['present_model_files'].append(model_file)
                    print(f"  âœ… Found: {model_file}")
                else:
                    comparison['missing_model_files'].append(model_file)
                    comparison['files_match'] = False
                    print(f"  âŒ Missing: {model_file}")
        
        return comparison
    
    def generate_report(self) -> str:
        """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report = []
        report.append("=" * 80)
        report.append("STREAMLIT vs API COMPARISON REPORT")
        report.append("=" * 80)
        report.append("")
        
        # å®Ÿè¡Œæƒ…å ±
        report.append("ğŸ“‹ EXECUTION INFO")
        report.append("-" * 40)
        report.append(f"Folder: {self.folder_name}")
        report.append(f"Model: {self.model_name}")
        report.append(f"TD: {self.td}")
        report.append(f"Max Distance: {self.max_distance_from_face}")
        report.append("")
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ
        metrics_comparison = self.comparison_report.get('metrics', {})
        report.append("ğŸ“Š METRICS COMPARISON")
        report.append("-" * 40)
        
        if metrics_comparison.get('metrics_match', False):
            report.append("âœ… METRICS MATCH PERFECTLY!")
        else:
            report.append("âŒ METRICS DO NOT MATCH!")
            
            for diff in metrics_comparison.get('differences', []):
                report.append(f"  â€¢ {diff['metric']}:")
                report.append(f"    Streamlit: {diff['streamlit']:.6f}")
                report.append(f"    API:       {diff['api']:.6f}")  
                report.append(f"    Diff:      {diff['difference']:.6f}")
        
        report.append("")
        
        # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        s_metrics = metrics_comparison.get('calculated_streamlit_metrics', {})
        a_metrics = metrics_comparison.get('calculated_api_metrics', {})
        
        for metric_type in ['settlement', 'convergence']:
            if metric_type in s_metrics:
                report.append(f"ğŸ“ˆ {metric_type.upper()} METRICS")
                report.append("-" * 40)
                s_m = s_metrics[metric_type]
                a_m = a_metrics.get(metric_type, {})
                
                for key in ['r2_train', 'r2_validate', 'mse_train', 'mse_validate']:
                    s_val = s_m.get(key, 'N/A')
                    a_val = a_m.get(key, 'N/A')
                    match_icon = "âœ…" if s_val == a_val else "âŒ"
                    
                    report.append(f"{match_icon} {key}:")
                    report.append(f"  Streamlit: {s_val}")
                    report.append(f"  API:       {a_val}")
                report.append("")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ¯”è¼ƒ
        files_comparison = self.comparison_report.get('files', {})
        report.append("ğŸ“ MODEL FILES CHECK")
        report.append("-" * 40)
        
        if files_comparison.get('files_match', False):
            report.append("âœ… ALL REQUIRED MODEL FILES PRESENT!")
        else:
            report.append("âŒ SOME MODEL FILES MISSING!")
            
            if files_comparison.get('missing_model_files'):
                report.append("Missing model files:")
                for file in files_comparison['missing_model_files']:
                    report.append(f"  â€¢ {file}")
        
        if files_comparison.get('present_model_files'):
            report.append("Present model files:")
            for file in files_comparison['present_model_files']:
                report.append(f"  âœ… {file}")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)
    
    def run_comparison(self):
        """å®Œå…¨ãªæ¯”è¼ƒã‚’å®Ÿè¡Œ"""
        print("ğŸš€ Starting Streamlit vs API Comparison")
        print("=" * 60)
        
        # 1. Streamlitåˆ†æå®Ÿè¡Œ
        self.streamlit_results = self.run_streamlit_analysis()
        
        # 2. APIåˆ†æå®Ÿè¡Œ
        self.api_results = self.run_api_analysis()
        
        # 3. çµæœæ¯”è¼ƒ
        if self.streamlit_results and self.api_results:
            self.comparison_report = {
                'metrics': self.compare_metrics(),
                'files': self.compare_files()
            }
            
            # 4. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = self.generate_report()
            print("\n" + report)
            
            # ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            report_file = "comparison_report.txt"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"\nğŸ“„ Report saved to: {report_file}")
            
            # çµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ï¼ˆå­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸€è‡´ + ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ã®ã¿ã§åˆ¤å®šï¼‰
            if (self.comparison_report['metrics'].get('metrics_match', False) and 
                self.comparison_report['files'].get('files_match', False)):
                print("\nğŸ‰ SUCCESS: Learning metrics match and all model files are present!")
                return True
            else:
                print("\nğŸ”§ ATTENTION: Either metrics don't match or model files are missing.")
                return False
        else:
            print("\nâŒ FAILED: Could not complete comparison")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    comparator = ResultComparator()
    success = comparator.run_comparison()
    
    if not success:
        print("\nğŸ› ï¸ NEXT STEPS:")
        print("1. Check the comparison report")
        print("2. Ensure learning metrics match exactly")
        print("3. Verify all required model files (.pkl) are generated")
        print("4. Re-run comparison")
    
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())